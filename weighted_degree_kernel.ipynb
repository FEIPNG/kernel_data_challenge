{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import cvxopt\n",
    "from scipy.optimize import minimize\n",
    "from collections import defaultdict\n",
    "\n",
    "def generate_kmers(sequence, k):\n",
    "    \"\"\"Generate all k-mers of a given sequence.\"\"\"\n",
    "    return [sequence[i:i+k] for i in range(len(sequence) - k + 1)]\n",
    "\n",
    "def weighted_degree_kernel(X1, X2, k_max, weights=None):\n",
    "    \"\"\"\n",
    "    Compute the Weighted Degree Kernel between two sets of sequences.\n",
    "    \n",
    "    Parameters:\n",
    "        X1, X2: Arrays of DNA sequences.\n",
    "        k_max: Maximum k-mer length.\n",
    "        weights: Weighting scheme for different k-mer lengths (default: higher k gets more weight).\n",
    "\n",
    "    Returns:\n",
    "        Kernel matrix of shape (len(X1), len(X2)).\n",
    "    \"\"\"\n",
    "    if weights is None:\n",
    "        weights = np.array([1 / (d + 1) for d in range(1, k_max + 1)])  # Decay weighting\n",
    "\n",
    "    kernel_matrix = np.zeros((len(X1), len(X2)))\n",
    "\n",
    "    for d in range(1, k_max + 1):  # Compute kernel for all k-mer sizes up to k_max\n",
    "        kmer_counts_X1 = [{generate_kmers(seq, d)[i]: i for i in range(len(seq) - d + 1)} for seq in X1]\n",
    "        kmer_counts_X2 = [{generate_kmers(seq, d)[i]: i for i in range(len(seq) - d + 1)} for seq in X2]\n",
    "\n",
    "        for i, seq1 in enumerate(X1):\n",
    "            for j, seq2 in enumerate(X2):\n",
    "                common_kmers = set(kmer_counts_X1[i]) & set(kmer_counts_X2[j])\n",
    "                kernel_matrix[i, j] += weights[d - 1] * sum(1 for km in common_kmers)\n",
    "\n",
    "    return kernel_matrix\n",
    "\n",
    "def sigmoid(z):\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "def logistic_loss(beta, K, y, C):\n",
    "    \"\"\"Logistic loss function for kernelized logistic regression.\"\"\"\n",
    "    m = len(y)\n",
    "    linear_term = K @ beta  # K * beta\n",
    "    loss = np.mean(np.log(1 + np.exp(-y * linear_term))) + (C / 2) * np.dot(beta, beta)\n",
    "    return loss\n",
    "\n",
    "def train_logistic_regression(K_train, y_train, C=1.0):\n",
    "    \"\"\"Train logistic regression with kernel trick using optimization.\"\"\"\n",
    "    beta_init = np.zeros(K_train.shape[0])\n",
    "\n",
    "    res = minimize(fun=logistic_loss, x0=beta_init, args=(K_train, y_train, C), method='L-BFGS-B')\n",
    "\n",
    "    return res.x  # Optimal beta\n",
    "\n",
    "def predict_logistic_regression(K_test, beta):\n",
    "    \"\"\"Make predictions using kernelized logistic regression.\"\"\"\n",
    "    probs = sigmoid(K_test @ beta)\n",
    "    return np.where(probs >= 0.5, 1, -1)\n",
    "\n",
    "def train_and_predict_weighted_degree_logistic(X_train_path, Y_train_path, X_test_path, k_max=6, C=1.0):\n",
    "    \"\"\"Pipeline for training and predicting with weighted degree kernel logistic regression.\"\"\"\n",
    "    # Load data\n",
    "    df_train = pd.read_csv(X_train_path)\n",
    "    df_labels = pd.read_csv(Y_train_path)\n",
    "    df_test = pd.read_csv(X_test_path)\n",
    "\n",
    "    X_train = df_train[\"seq\"].values\n",
    "    y_train = np.where(df_labels[\"Bound\"] == 1, 1, -1)  # Convert labels to {-1,1}\n",
    "    X_test = df_test[\"seq\"].values\n",
    "\n",
    "    # Compute kernel matrices\n",
    "    K_train = weighted_degree_kernel(X_train, X_train, k_max)\n",
    "    K_test = weighted_degree_kernel(X_test, X_train, k_max)\n",
    "\n",
    "    # Train logistic regression model\n",
    "    beta = train_logistic_regression(K_train, y_train, C)\n",
    "\n",
    "    # Make predictions\n",
    "    predictions = predict_logistic_regression(K_test, beta)\n",
    "\n",
    "    # Convert {-1,1} predictions to {0,1}\n",
    "    df_test[\"Bound\"] = (predictions + 1) // 2\n",
    "\n",
    "    return df_test\n",
    "\n",
    "# === Run the function ===\n",
    "df_predictions = train_and_predict_weighted_degree_logistic(\"./data/Xtr0.csv\", \"./data/Ytr0.csv\", \"./data/Xte0.csv\", k_max=6, C=1.0\")\n",
    "\n",
    "# Save predictions\n",
    "df_predictions.to_csv(\"predictions.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why This Might Improve Performance\n",
    "\n",
    "✅ Better than Spectrum Kernel:\n",
    "\n",
    "    Spectrum Kernel ignores sequence order, while the Weighted Degree Kernel captures structural patterns in DNA.\n",
    "✅ More Robust to Small Variations:\n",
    "    By weighting longer k-mers more, it helps capture longer-term dependencies in the sequence.\n",
    "    \n",
    "✅ Less Sensitive to Overfitting:\n",
    "    The decay weighting penalizes very short patterns, reducing noise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
